{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tree Machine: An AutoML companion to fit tree models easily","text":"<p>This package aims to give users a simple interface to fit tree models. Tree models are the workhorse for tabular data and are used in many applications. Our aim is to simplify the use, tuning and deployment of these models.</p>"},{"location":"#autotrees","title":"AutoTrees","text":"<p>Specific auto-tune trees that use Bayesian optimization to select the best model overall and the best hyperparameters for that model. The models are trained using a <code>xgboost</code> backend, and the user can change the parameters to use during <code>fit</code>.</p> <p>Can be used as a last step inside a <code>sklearn.pipeline</code> object.</p>"},{"location":"#installing-the-package","title":"Installing the package","text":"<p>Just issue the command:</p> <pre><code>python -m pip install bezzanlabs.tree_machine\n</code></pre> <p>This package should work in all systems, and it was tested in Linux and MacOS.</p>"},{"location":"#setup-for-development","title":"Setup for development","text":"<p>To install this package, run the following command in your terminal:</p> <pre><code>make install\n</code></pre> <p>This should proceed with the installation of all dependencies for development.</p>"},{"location":"base/","title":"base.py","text":""},{"location":"base/#summary","title":"Summary","text":"<p>BaseAutoCV is an abstract base class for implementing automated cross-validation and hyperparameter optimization using Bayesian optimization.</p>"},{"location":"base/#dependencies","title":"Dependencies","text":""},{"location":"base/#standard-library","title":"Standard Library","text":"<ul> <li>abc</li> <li>typing</li> </ul>"},{"location":"base/#other","title":"Other","text":"<ul> <li>numpy</li> <li>pandas</li> <li>optuna</li> <li>pydantic</li> <li>sklearn</li> </ul>"},{"location":"base/#description","title":"Description","text":"<p>The <code>BaseAutoCV</code> class is an abstract base class that provides a framework for implementing automated cross-validation and hyperparameter optimization using Bayesian optimization. It inherits from both <code>ABC</code> (Abstract Base Class) and <code>BaseEstimator</code> from scikit-learn.</p> <p>The class is designed to be subclassed and not instantiated directly. It provides a structure for creating estimators that can automatically find optimal hyperparameters using Optuna, a hyperparameter optimization framework. The class includes methods for prediction, explanation, and optimization, as well as properties for accessing the optimization study and cross-validation results.</p> <p>The main functionality is implemented in the <code>optimize</code> method, which uses Optuna to perform Bayesian optimization of the model hyperparameters. It creates a study object and optimizes an objective function that performs cross-validation for each trial. The best parameters found during the optimization process are stored and used to fit the final model.</p> <p>The class also includes utility methods for validating input data (<code>_validate_X</code> and <code>_validate_y</code>) and abstract methods that need to be implemented by subclasses (<code>predict</code>, <code>predict_proba</code>, <code>scorer</code>, and <code>explain</code>). These methods ensure proper data handling and provide a consistent interface for different types of models.</p> <p>This documentation was generated using claude-3-5-sonnet-20240620</p>"},{"location":"classification_metrics/","title":"classification_metrics.py","text":""},{"location":"classification_metrics/#summary","title":"Summary","text":"<p>This code defines a collection of classification metrics and provides a validation mechanism for acceptable classifier names.</p>"},{"location":"classification_metrics/#dependencies","title":"Dependencies","text":""},{"location":"classification_metrics/#standard-library","title":"Standard Library","text":"<ul> <li>functools</li> <li>typing_extensions</li> </ul>"},{"location":"classification_metrics/#other","title":"Other","text":"<ul> <li>sklearn.metrics</li> <li>pydantic</li> </ul>"},{"location":"classification_metrics/#description","title":"Description","text":"<p>This Python module provides a comprehensive set of classification metrics commonly used in machine learning tasks. It leverages the scikit-learn library to access various scoring functions such as F1 score, precision, and recall.</p> <p>The <code>classification_metrics</code> dictionary is the core of this module. It maps human-readable metric names to their corresponding scikit-learn functions. For each metric type (F1, precision, recall), it includes both the default version and variants with different averaging methods (macro, micro, samples, weighted). This allows for easy access to a wide range of classification evaluation metrics.</p> <p>The module also defines a custom validation function <code>_is_classification_metric</code> which checks if a given metric name is present in the <code>classification_metrics</code> dictionary. This function is used in conjunction with Pydantic's <code>AfterValidator</code> to create an <code>AcceptableClassifier</code> type. This type can be used to ensure that only valid classification metric names are accepted in other parts of the codebase that might use this module.</p> <p>By providing a centralized collection of classification metrics and a validation mechanism, this module promotes consistency and error prevention when working with classification tasks in a larger machine learning project.</p> <p>This documentation was generated using claude-3-5-sonnet-20240620</p>"},{"location":"classifier_cv/","title":"classifier_cv.py","text":""},{"location":"classifier_cv/#summary","title":"Summary","text":"<p>This code defines a <code>ClassifierCV</code> class for automated classification using XGBoost with Bayesian optimization and SHAP explanations.</p>"},{"location":"classifier_cv/#dependencies","title":"Dependencies","text":""},{"location":"classifier_cv/#standard-library","title":"Standard Library","text":"<ul> <li>typing</li> </ul>"},{"location":"classifier_cv/#other","title":"Other","text":"<ul> <li>numpy</li> <li>pandas</li> <li>pydantic</li> <li>shap</li> <li>sklearn</li> <li>xgboost</li> </ul>"},{"location":"classifier_cv/#description","title":"Description","text":"<p>The <code>classifier_cv.py</code> file implements an automated classification system using XGBoost as the base classifier. The main class, <code>ClassifierCV</code>, inherits from <code>BaseAutoCV</code>, <code>ClassifierMixin</code>, and <code>ExplainerMixIn</code>, combining functionality for automated model training, classification, and model explanation.</p> <p>The system uses Bayesian optimization to find the best hyperparameters for the XGBoost classifier. It allows for configuration of monotonicity constraints and interaction constraints on features, which can be useful for enforcing domain knowledge or logical relationships in the data.</p> <p>The <code>ClassifierCV</code> class provides methods for fitting the model, making predictions, and explaining the model's decisions using SHAP (SHapley Additive exPlanations) values. It also includes functionality for cross-validation and custom metric optimization.</p> <p>The code is designed to be flexible and customizable, allowing users to specify their own classification metrics, cross-validation strategies, and optimization parameters. It also includes type hints and validation using Pydantic, which enhances code reliability and provides clear interface definitions.</p> <p>This documentation was generated using claude-3-5-sonnet-20240620</p>"},{"location":"explainer/","title":"ExplainerMixIn","text":"<p>Here's the markdown documentation for the provided code:</p>"},{"location":"explainer/#explainerpy","title":"explainer.py","text":""},{"location":"explainer/#summary","title":"Summary","text":"<p>Defines an abstract base class <code>ExplainerMixIn</code> for implementing explainability using SHAP values.</p>"},{"location":"explainer/#dependencies","title":"Dependencies","text":""},{"location":"explainer/#standard-library","title":"Standard Library","text":"<ul> <li>abc</li> </ul>"},{"location":"explainer/#other","title":"Other","text":"<ul> <li>numpy</li> <li>shap</li> </ul>"},{"location":"explainer/#description","title":"Description","text":"<p>This file defines an abstract base class <code>ExplainerMixIn</code> that provides a minimal interface for implementing explainability in machine learning models using SHAP (SHapley Additive exPlanations) values. The class is designed to be used as a mixin, allowing other classes to inherit its functionality.</p> <p>The <code>ExplainerMixIn</code> class requires implementing classes to have an <code>explainer_</code> attribute of type <code>shap.Explainer</code>. This attribute is expected to be initialized with a SHAP explainer object, which will be used to generate explanations for model predictions.</p> <p>The class defines an abstract method <code>explain</code> that takes input data <code>X</code> and optional explainer parameters. This method is intended to return a dictionary containing the average response values and their respective SHAP values for each class in the case of a multi-class classifier. Implementing classes must provide their own implementation of this method to generate explanations specific to their model architecture and use case.</p> <p>By using this mixin, developers can easily add explainability features to their machine learning models, providing insights into how the model makes predictions and which features contribute most to those predictions.</p> <p>This documentation was generated using claude-3-5-sonnet-20240620</p>"},{"location":"license/","title":"License","text":"<p>Copyright 2023-2024 Vitor Pereira Bezzan</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"optimizer_params/","title":"optimizer_params.py","text":""},{"location":"optimizer_params/#summary","title":"Summary","text":"<p>This code defines a class for managing hyperparameter configurations in optimization tasks.</p>"},{"location":"optimizer_params/#dependencies","title":"Dependencies","text":""},{"location":"optimizer_params/#standard-library","title":"Standard Library","text":"<ul> <li>None</li> </ul>"},{"location":"optimizer_params/#other","title":"Other","text":"<ul> <li>optuna</li> </ul>"},{"location":"optimizer_params/#description","title":"Description","text":"<p>The <code>optimizer_params.py</code> file contains a single class, <code>OptimizerParams</code>, which is designed to handle hyperparameter configurations for optimization tasks, particularly for machine learning models. This class is especially useful when working with optimization frameworks like Optuna.</p> <p>The <code>OptimizerParams</code> class defines a set of hyperparameters and their respective ranges or possible values in the <code>hyperparams_grid</code> class attribute. These hyperparameters are typically used in tree-based models, such as XGBoost. The grid includes parameters like learning rate (<code>eta</code>), regularization parameters (<code>reg_alpha</code>, <code>reg_lambda</code>), tree-specific parameters (<code>max_depth</code>, <code>n_estimators</code>), and various sampling parameters.</p> <p>The class provides a method <code>get_trial_values</code> that takes an Optuna <code>Trial</code> object as input and returns a dictionary of suggested hyperparameter values. This method dynamically generates parameter suggestions based on the type and range defined in the <code>hyperparams_grid</code>. It supports float, integer, and categorical parameters, making it flexible for various hyperparameter types.</p> <p>This implementation allows for easy integration with Optuna's optimization process, enabling efficient hyperparameter tuning for machine learning models. The class can be extended or modified to include additional hyperparameters or adjust the ranges as needed for specific optimization tasks.</p> <p>This documentation was generated using claude-3-5-sonnet-20240620</p>"},{"location":"regression_cv/","title":"regression_cv.py","text":""},{"location":"regression_cv/#summary","title":"Summary","text":"<p>This code defines a RegressionCV class for automated regression tree modeling using Bayesian optimization.</p>"},{"location":"regression_cv/#dependencies","title":"Dependencies","text":""},{"location":"regression_cv/#standard-library","title":"Standard Library","text":"<ul> <li>typing</li> </ul>"},{"location":"regression_cv/#other","title":"Other","text":"<ul> <li>numpy</li> <li>pandas</li> <li>pydantic</li> <li>shap</li> <li>sklearn</li> <li>xgboost</li> </ul>"},{"location":"regression_cv/#description","title":"Description","text":"<p>The <code>regression_cv.py</code> file implements a <code>RegressionCV</code> class, which is an automated regression tree model based on Bayesian optimization. This class combines several machine learning techniques to create a powerful and flexible regression model.</p> <p>The <code>RegressionCV</code> class inherits from <code>BaseAutoCV</code>, <code>RegressorMixin</code>, and <code>ExplainerMixIn</code>, providing a comprehensive set of functionalities for regression tasks, model optimization, and result explanation. It uses XGBoost as the underlying regression algorithm and incorporates Shapley Additive Explanations (SHAP) for model interpretability.</p> <p>The class allows for customization through a <code>RegressionCVConfig</code> dataclass, which includes options for monotonicity constraints, feature interactions, hyperparameter search spaces, and parallelization. The model optimization process uses Bayesian optimization to find the best hyperparameters within the specified search space.</p> <p>Key features of the <code>RegressionCV</code> class include: 1. Automated hyperparameter tuning using Bayesian optimization 2. Support for custom evaluation metrics 3. Cross-validation during model selection 4. Feature importance calculation 5. Model explanation using SHAP values</p> <p>The class provides methods for fitting the model, making predictions, and explaining the model's decisions. It also includes utility methods for input validation and scoring.</p> <p>This documentation was generated using claude-3-5-sonnet-20240620</p>"},{"location":"regression_metrics/","title":"regression_metrics.py","text":""},{"location":"regression_metrics/#summary","title":"Summary","text":"<p>This code defines and validates regression metrics for machine learning evaluation.</p>"},{"location":"regression_metrics/#dependencies","title":"Dependencies","text":""},{"location":"regression_metrics/#standard-library","title":"Standard Library","text":"<ul> <li>typing_extensions</li> </ul>"},{"location":"regression_metrics/#other","title":"Other","text":"<ul> <li>sklearn.metrics</li> <li>pydantic</li> </ul>"},{"location":"regression_metrics/#description","title":"Description","text":"<p>This Python module provides a collection of commonly used regression metrics for evaluating machine learning models. It imports specific metric functions from scikit-learn's metrics module and organizes them into a dictionary for easy access and use.</p> <p>The <code>regression_metrics</code> dictionary maps string keys to their corresponding metric functions. The available metrics include Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Median Absolute Error, and Mean Squared Error (MSE).</p> <p>To ensure that only valid regression metrics are used, the module defines a custom validation function <code>_is_regression_metric</code>. This function checks if a given metric name is present in the <code>regression_metrics</code> dictionary and raises an assertion error if it's not.</p> <p>The module also utilizes Pydantic's <code>AfterValidator</code> to create an <code>AcceptableRegression</code> type. This type annotation can be used to validate input parameters in other parts of the codebase, ensuring that only valid regression metric names are accepted.</p> <p>This documentation was generated using claude-3-5-sonnet-20240620</p>"},{"location":"types/","title":"types.py","text":""},{"location":"types/#summary","title":"Summary","text":"<p>This file defines custom type aliases for input data, ground truth, and predictions used in the package.</p>"},{"location":"types/#dependencies","title":"Dependencies","text":""},{"location":"types/#standard-library","title":"Standard Library","text":"<ul> <li>None</li> </ul>"},{"location":"types/#other","title":"Other","text":"<ul> <li>numpy</li> <li>pandas</li> </ul>"},{"location":"types/#description","title":"Description","text":"<p>The <code>types.py</code> file provides type definitions that are used throughout the package to ensure type consistency and improve code readability. It defines three main type aliases:</p> <ol> <li> <p><code>Inputs</code>: Represents input data, which can be either a NumPy array of float64 values or a pandas DataFrame.</p> </li> <li> <p><code>GroundTruth</code>: Represents ground truth data, which can be either a NumPy array of float64 values or a pandas Series.</p> </li> <li> <p><code>Predictions</code>: Represents prediction data, which is defined as a NumPy array of float64 values.</p> </li> </ol> <p>These type aliases are particularly useful in machine learning and data analysis contexts, where input data, ground truth labels, and model predictions are common entities. By using these type aliases, the package can maintain consistent type hints across different modules and functions, enhancing code clarity and facilitating static type checking.</p> <p>This documentation was generated using claude-3-5-sonnet-20240620</p>"}]}